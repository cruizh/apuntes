[
["index.html", "Modelos Lineales y Diseño de Experimentos Requisitos previos", " Modelos Lineales y Diseño de Experimentos Carlos José Ruiz-Henestrosa Ruiz 2018-02-08 Requisitos previos Este libro está escrito en Markdown, concretamente el dialecto de Pandoc. Puedes consultar el código fuente que ha generado este sitio web en el repositorio de GitHub. o pulsando el botón de edición de la barra superior. La compilación de este libro se ha realizado mediante bookdown. Se puede instalar desde CRAN o GitHub: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) "],
["informacion-de-la-asignatura.html", "Información de la asignatura 0.1 Profesores 0.2 Bibliografía recomendada 0.3 Evaluación alternativa", " Información de la asignatura 0.1 Profesores Las clases prácticas tendrán lugar los martes en el Laboratorio 1 (alumnos con DNI impar) o en el Laboratorio 2 (alumnos con DNI par) y serán impartidas por Joaquín García de las Heras. Las clases teóricas tendrán lugar los jueves en el aula H1.11 y serán impartidas por Emilio Carrizosa. 0.2 Bibliografía recomendada (Harrell 2001) (Friedman, Hastie, y Tibshirani 2001) (Montgomery 1991) (Peña 2002) 0.3 Evaluación alternativa La asignatura se divide en una parte de teoría y otra de problemas. La prueba alternativa de teoría tendrá lugar el último día de clase. Se realizarán al menos 2 pruebas de problemas en días asignados para prácticas. Los exámenes de problemas se realizarán en ordenador debido a la naturaleza de los cálculos necesarios. Bibliografía "],
["ml-regresion.html", "Tema 1 Modelos lineales en regresión 1.1 Regresión lineal (simple)", " Tema 1 Modelos lineales en regresión 1.1 Regresión lineal (simple) La regresión lineal simple consiste en aproximar los valores que toman una sucesión \\((X_1, Y_1) \\dots, (X_n, Y_n)\\) independientes pero tal que \\(Y_i\\) y \\(X_i\\) son dependientes mediante la siguiente expresión: \\[ y = \\beta_{0} + \\beta_{1} x + \\varepsilon \\] Sea \\(g(\\beta_0, \\beta_1) = \\min \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\beta_1 x_i\\right)^{2}\\). La obtención de los \\(\\beta_i\\) se realiza aprovechando la condición necesaria y suficiente de optimalidad: \\[ \\frac{\\partial}{\\partial \\beta_0} g(\\beta) = \\frac{\\partial}{\\partial \\beta_1} g(\\beta) = 0 \\] \\[\\begin{gather*} \\left. \\begin{aligned} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) &amp;= 0\\\\ \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)x_i &amp;= 0 \\end{aligned} \\right\\}\\\\ \\left. \\begin{aligned} n \\overline{y} - n\\beta_0 - n\\beta_1 \\overline{x} &amp;= 0\\\\ n \\overline{yx} - n\\beta_0 \\overline{x} - n \\beta_1 \\overline{x^{2}} &amp;= 0 \\end{aligned} \\right\\}\\\\ \\beta_0 = \\overline{y} - \\beta_1 \\overline{x} \\end{gather*}\\] \\[ \\left. \\begin{aligned} \\quad \\overline{y} - \\beta_0 - \\beta_1 \\overline{x} &amp;= 0 \\quad (1)\\\\ \\quad \\overline{yx}- \\beta_0 \\overline{x} - \\beta_1 \\overline{x^{2}} &amp;= 0 \\quad (2) \\end{aligned} \\right\\} \\] Si hacemos \\((2) - x(1)\\), obtenemos \\[\\begin{gather*} \\underbrace{\\overline{yx} - \\overline{y}\\, \\overline{x}}_{S_{X,Y}} - \\beta_1 \\underbrace{\\left( \\overline{x^{2}} - \\overline{x}^{2} \\right)}_{S_{X}^{2}} = 0\\\\ \\implies \\beta_1 = \\frac{S_{X,Y}}{S_{X}^{2}}, \\quad \\beta_0 = \\overline{y} - \\frac{S_{X,Y}}{S_{X}^{2}} \\overline{x} \\end{gather*}\\] "],
["bibliografia.html", "Bibliografía", " Bibliografía "]
]
